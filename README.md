ðŸš€ Project: "GitHub" - Real-Time Event Pipeline
This project is an end-to-end streaming data pipeline that captures, processes, and stores public events from the GitHub API in real-time. The goal was to build a robust, scalable, and fault-tolerant architecture using industry-standard data engineering tools.

Core Stack: Apache Airflow | Apache Kafka | Apache Spark | Docker | Python

ðŸ—ï¸ Pipeline Architecture
This project implements a "decoupled" architecture, meaning Kafka acts as a central buffer to ensure no data is lost, even if parts of the system fail.

The data flow works as follows:

Orchestration (Airflow): An Apache Airflow DAG is scheduled to run every 5 minutes.

Production (Python + Docker): The DAG uses the DockerOperator to spin up a dedicated Python container (producer.py). This script calls the GitHub Events API, authenticates, and fetches the latest events.

Messaging (Kafka): The script sends each event (JSON) as a separate message to an Apache Kafka topic (github_events_raw).

Consumption (Spark Streaming): A Spark Structured Streaming job (consumer.py) listens to the Kafka topic 24/7.

Transformation (ETL): As data arrives, Spark processes it in micro-batches:

Converts the raw JSON.

Defines and applies a schema (structure) to the data.

Flattens the nested JSON data (e.g., actor.login, repo.name).

Creates partitioning columns (year, month, day).

Loading (Data Lake): The clean, transformed data is saved in Parquet format to a local volume, partitioned by date, creating an analysis-ready Data Lake.

âš™ï¸ Tech Stack
Each tool was chosen for its specific role in the architecture:

Docker & Docker Compose

Role: Virtualization & Environment.

Why: It containerizes every service (Kafka, Spark, Airflow, etc.), ensuring the development environment is identical to production and 100% portable.

Apache Airflow

Role: Task Orchestration.

Why: Reliably schedules and executes the data collection task (the Producer) at regular intervals.

Apache Kafka

Role: Message Bus (Buffer).

Why: The heart of fault tolerance. If the Spark (Consumer) fails for 1 hour, the Producer continues to send data to Kafka. When Spark comes back online, it resumes from where it left off with no data loss.

Apache Spark

Role: Streaming Data Processing.

Why: A leading tool for large-scale ETL. Structured Streaming allows for efficient real-time (micro-batch) data processing.

Python

Role: The "glue" of the project.

Why: Used to write the Producer (producer.py) and Consumer (consumer.py via PySpark) scripts.

Parquet Format

Role: Optimized Storage (Data Lake).

Why: A highly compressed, columnar format ideal for fast analytical queries.

ðŸŒ³ Project Structure
Here is the file tree, explaining what each file and folder does.

github_pulse1/
â”œâ”€â”€ .env                  # Environment file (ignored by Git)
â”œâ”€â”€ .gitignore            # Tells Git which files to ignore (tokens, logs, data)
â”œâ”€â”€ docker-compose.yml    # The "brain" of Docker, defines all services
â”œâ”€â”€ README.md             # This documentation
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ github_events_dag.py  # The DAG that orchestrates the producer
â”‚   â”œâ”€â”€ logs/                 # (Ignored) Logs generated by Airflow
â”‚   â””â”€â”€ plugins/              # (Ignored) Airflow plugins
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ Dockerfile            # Recipe to build the producer image
â”‚   â”œâ”€â”€ producer.py           # Python script that calls the API and sends to Kafka
â”‚   â””â”€â”€ requirements.txt      # Libraries (requests, kafka-python)
â”‚
â””â”€â”€ spark/
    â”œâ”€â”€ app/
    â”‚   â””â”€â”€ consumer.py       # PySpark script that reads from Kafka and saves to Parquet
    â””â”€â”€ data/
        â”œâ”€â”€ checkpoints/      # (Ignored) Spark Streaming "bookmarks"
        â””â”€â”€ processed/        # (Ignored) Where the .parquet files are saved

        
ðŸš€ðŸ•µï¸ How to Run This Project
Follow these steps to configure and run the entire pipeline on your local machine.

1. Prerequisites
Docker Desktop (for Windows/Mac).

A GitHub Personal Access Token (PAT).

Go to GitHub > Settings > Developer settings > Tokens (classic).

Generate a new token with the public_repo scope.

2. Environment Setup (Windows Only)
This project has been debugged to solve specific Windows issues with Docker:

Expose the Docker Daemon:

Open Docker Desktop Settings.

Go to the "General" section.

CHECK the box: "Expose daemon on tcp://localhost:2375 without TLS".

Click "Apply & Restart".

Verify the Airflow DAG:

The airflow/dags/github_events_dag.py file must use docker_url='tcp://host.docker.internal:2375' in the DockerOperator.

3. Project Setup
First, clone the repository:

git clone https://github.com/jovesoncosta/streaming-data-pipeline-kafka-spark-airflow.git cd your-repository

Create a file named .env in the project root and add the AIRFLOW_UID (for permissions):

AIRFLOW_UID=50000

(Your GITHUB_TOKEN will be configured directly in the Airflow UI for better security.)

<img width="555" height="260" alt="ENV" src="https://github.com/user-attachments/assets/f861c5a5-22ef-4948-a90d-89fb5dc1ecc0" />


Finally, build the Docker image that Airflow will use to run the producer:

docker build -t github_producer:latest ./producer

4. Starting the Pipeline (Command Sequence)
1. Start All Services:

docker-compose up -d

2. Configure the Airflow Variable:

Wait 1-2 minutes for Airflow to start.

Access the UI: http://localhost:8081 (login: admin / pass: admin).

Go to "Admin" -> "Variables".

Click + to add a new variable:

Key: GITHUB_TOKEN

Val: Paste your GitHub Token (e.g., ghp_...).

Click "Save".

3. Create the Kafka Topic: The topic must be created manually, as docker-compose down deletes it.

docker-compose exec kafka kafka-topics --create --topic github_events_raw --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1

4. Start the Consumer (Spark):

Open a new terminal (keep it open).

Run the Spark job. It will "listen" to the Kafka topic.

docker-compose exec --user root spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 /opt/spark/work-dir/app/consumer.py

5. Start the Producer (Airflow):

With Spark "listening," go back to the Airflow UI.

Enable the github_events_producer DAG (click the "play" toggle).

Trigger it manually by clicking the "play" button > "Trigger DAG".

<img width="1887" height="472" alt="dagss" src="https://github.com/user-attachments/assets/99b55074-1080-4571-b85f-033afac92cda" />



5. Verify the Results
Spark Terminal: You will see the micro-batches being printed to the terminal (from the format("console") sink).

<img width="853" height="476" alt="batch" src="https://github.com/user-attachments/assets/61beb5fe-f9a6-403f-9f18-052afc350cc3" />



DataLake: Check the spark/data/processed/events/ folder in your project. The year, month, and day folders will be created and will contain your .parquet files!
